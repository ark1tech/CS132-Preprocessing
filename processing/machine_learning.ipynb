{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies and set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import spacy    \n",
    "import spacy_transformers\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn import decomposition\n",
    "import numpy as np \n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px # pip install plotly\n",
    "import matplotlib.pyplot as plt # pip install matplotlib\n",
    "%matplotlib inline\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'\n",
    "import textwrap\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "df = pd.read_csv(\"../dataset/_compiled/Clustered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, TF-IDF Vectorization, LDA Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(cell):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", cell) \n",
    "\n",
    "def my_tokenizer(text):\n",
    "    # 1-gram tokens of more than a character\n",
    "    word_tokens = [x for x in word_tokenize(text) if len(x) > 2]\n",
    "    # remove stop words \n",
    "    filtered_tokens = [word for word in word_tokens if word not in set([remove_punctuation(x) for x in [*stopwords.words('english'), *nlp.Defaults.stop_words, *[str(x) for x in open(\"stop_words.txt\", \"r\").read().split(\" \")]]])]\n",
    "    # lemmatize the tokens \n",
    "    # lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer,\n",
    "                                min_df = 3, max_df=0.8, max_features=25000, \n",
    "                                use_idf=True, norm=None, token_pattern=None)\n",
    "tf_vectors = tf_vectorizer.fit_transform(df[\"Translated\"])  \n",
    "\n",
    "# Topic Modelling using LDA\n",
    "\n",
    "n_topics = 5\n",
    "lda = decomposition.LatentDirichletAllocation(n_components=n_topics, max_iter=35, \n",
    "                                              learning_method='online', learning_offset=55, n_jobs=1, random_state=420)\n",
    "W = lda.fit_transform(tf_vectors)\n",
    "H = lda.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 100 relevant words for each topic\n",
    "\n",
    "num_words = 50\n",
    "vocab = np.array(tf_vectorizer.get_feature_names_out())\n",
    "top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "topic_words = ([top_words(t) for t in H])\n",
    "topics = [' '.join(t) for t in topic_words]\n",
    "df_topics = pd.DataFrame(topics, columns=['Keywords'])\n",
    "df_topics['Topic ID'] = range(1, len(topics) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>breakdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Submission 1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1: 0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3: 0.61\\nTopic 5: 0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Topic 5: 0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>Topic 4: 0.59\\nTopic 2: 0.33\\nTopic 3: 0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 5</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>Topic 5: 0.86\\nTopic 1: 0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  dominant_topic  \\\n",
       "Submission 1     0.99     0.00     0.00     0.00     0.00               1   \n",
       "Submission 2     0.00     0.00     0.61     0.00     0.39               3   \n",
       "Submission 3     0.00     0.00     0.00     0.00     0.99               5   \n",
       "Submission 4     0.00     0.33     0.08     0.59     0.00               4   \n",
       "Submission 5     0.13     0.00     0.00     0.00     0.86               5   \n",
       "\n",
       "                                                breakdown  \n",
       "Submission 1                                Topic 1: 0.99  \n",
       "Submission 2                 Topic 3: 0.61\\nTopic 5: 0.39  \n",
       "Submission 3                                Topic 5: 0.99  \n",
       "Submission 4  Topic 4: 0.59\\nTopic 2: 0.33\\nTopic 3: 0.08  \n",
       "Submission 5                 Topic 5: 0.86\\nTopic 1: 0.13  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign topic to each submision\n",
    "topicid = [\"Topic \" + str(i+1) for i in range(lda.n_components)]\n",
    "postid = [\"Submission \" + str(i+1) for i in range(len(df[\"Translated\"]))]\n",
    "\n",
    "df_topics_lda = pd.DataFrame(np.round(W,2), columns=topicid, index=postid)\n",
    "significanttopic = np.argmax(df_topics_lda.values, axis=1)+1\n",
    "\n",
    "df_topics_lda['dominant_topic'] = significanttopic\n",
    "df_topics_lda['breakdown'] = df_topics_lda.apply(lambda row: '\\n'.join([f'{col}: {row[col]}' \n",
    "                                                        for col in sorted(df_topics_lda.columns, key=lambda x: row[x], reverse=True) \n",
    "                                                        if row[col] > 0 and col != 'dominant_topic']), axis=1)\n",
    "df_topics_lda.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Clustering and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=120, perplexity=37, angle=0.8)\n",
    "tsne_result = tsne.fit_transform(df_topics_lda.iloc[:,:n_topics])\n",
    "\n",
    "# Create a new dataframe with t-SNE coordinates and cluster labels\n",
    "def split_text(text, max_length):\n",
    "  lines = textwrap.wrap(text, width=max_length, break_long_words=False)\n",
    "  return \"<br>\".join(lines)\n",
    "\n",
    "df_topics_cluster = pd.DataFrame({\n",
    "                                  'LDA Topic': df_topics_lda.reset_index()['dominant_topic'].astype(str), # topics via LDA\n",
    "                                  'Hovertext': df[\"Translated\"],\n",
    "                                  'Breakdown': df_topics_lda.reset_index()['breakdown'],\n",
    "                                  'X': tsne_result[:, 0],\n",
    "                                  'Y': tsne_result[:, 1]\n",
    "                                })\n",
    "\n",
    "df_topics_cluster['Hovertext'] = df_topics_cluster['Hovertext'].apply(lambda x: split_text(x, 40))\n",
    "df_topics_cluster['Breakdown'] = df_topics_cluster['Breakdown'].str.replace('\\n','<br>')\n",
    "\n",
    "df_topics_cluster = pd.concat([df, df_topics_cluster], axis=1)\n",
    "\n",
    "# df_top_10 = df_topics_cluster.sort_values('Engagements', ascending=False).groupby('LDA Topic').head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
