{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing dependencies and set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ray/Documents/Obsidian/Arki's Vault/2_CS 132/_Code/scraper/.scrape/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/ray/Documents/Obsidian/Arki's Vault/2_CS 132/_Code/scraper/.scrape/lib/python3.12/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/ray/Documents/Obsidian/Arki's Vault/2_CS 132/_Code/scraper/.scrape/lib/python3.12/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import spacy    \n",
    "import spacy_transformers\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn import decomposition\n",
    "import numpy as np \n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import plotly.express as px # pip install plotly\n",
    "import matplotlib.pyplot as plt # pip install matplotlib\n",
    "%matplotlib inline\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'browser'\n",
    "import textwrap\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # or 1000\n",
    "pd.set_option('display.max_rows', None)  # or 1000\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "\n",
    "df = pd.read_csv(\"../dataset/_compiled/Clustered.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization, TF-IDF Vectorization, LDA Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(cell):\n",
    "    return re.sub(r\"[^\\w\\s]\", \"\", cell) \n",
    "\n",
    "def my_tokenizer(text):\n",
    "    # 1-gram tokens of more than a character\n",
    "    word_tokens = [x for x in word_tokenize(text) if len(x) > 2]\n",
    "    # remove stop words \n",
    "    filtered_tokens = [word for word in word_tokens if word not in set([remove_punctuation(x) for x in [*stopwords.words('english'), *nlp.Defaults.stop_words, *[str(x) for x in open(\"stop_words.txt\", \"r\").read().split(\" \")]]])]\n",
    "    # lemmatize the tokens \n",
    "    # lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          Keywords  \\\n",
      "0      pay company resignation department contract employment labor employer human resource letter leave resigned clearance immediate need render coe email period   \n",
      "1  people business job companies philippines outsourcing company workers life minimum salary working employees good wage corporate jobs christmas industry culture   \n",
      "2      interview job offer company human resource salary applied position experience hiring application companies time process role recruiter red apply applicants   \n",
      "3         boss team manager time supervisor hours tasks management coworkers people office company person performance meeting bad holiday angry colleague managers   \n",
      "4                                      leave job time company office home sick working vacation hours boss salary resign money shift team need long pay experience   \n",
      "\n",
      "   Topic ID  \n",
      "0         1  \n",
      "1         2  \n",
      "2         3  \n",
      "3         4  \n",
      "4         5  \n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
    "\n",
    "# Vectorize using TF-IDF\n",
    "\n",
    "tf_vectorizer = TfidfVectorizer(tokenizer=my_tokenizer,\n",
    "                                min_df = 3, max_df=0.8, max_features=25000, \n",
    "                                use_idf=True, norm=None, token_pattern=None)\n",
    "tf_vectors = tf_vectorizer.fit_transform(df[\"Translated\"])  \n",
    "\n",
    "# Topic Modelling using LDA\n",
    "\n",
    "n_topics = 5\n",
    "lda = decomposition.LatentDirichletAllocation(n_components=n_topics, max_iter=35, \n",
    "                                              learning_method='online', learning_offset=55, n_jobs=1, random_state=420)\n",
    "W = lda.fit_transform(tf_vectors)\n",
    "H = lda.components_\n",
    "\n",
    "# Show top 15 relevant words for each topic\n",
    "\n",
    "num_words = 20\n",
    "vocab = np.array(tf_vectorizer.get_feature_names_out())\n",
    "top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_words-1:-1]]\n",
    "topic_words = ([top_words(t) for t in H])\n",
    "topics = [' '.join(t) for t in topic_words]\n",
    "df_topics = pd.DataFrame(topics, columns=['Keywords'])\n",
    "df_topics['Topic ID'] = range(1, len(topics) + 1)\n",
    "print(df_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topics.to_csv(f\"../dataset/_compiled/Keywords.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>dominant_topic</th>\n",
       "      <th>breakdown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Submission 1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>Topic 1: 0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 2</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3</td>\n",
       "      <td>Topic 3: 0.61\\nTopic 5: 0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.99</td>\n",
       "      <td>5</td>\n",
       "      <td>Topic 5: 0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4</td>\n",
       "      <td>Topic 4: 0.59\\nTopic 2: 0.33\\nTopic 3: 0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Submission 5</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>Topic 5: 0.86\\nTopic 1: 0.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Topic 1  Topic 2  Topic 3  Topic 4  Topic 5  dominant_topic  \\\n",
       "Submission 1     0.99     0.00     0.00     0.00     0.00               1   \n",
       "Submission 2     0.00     0.00     0.61     0.00     0.39               3   \n",
       "Submission 3     0.00     0.00     0.00     0.00     0.99               5   \n",
       "Submission 4     0.00     0.33     0.08     0.59     0.00               4   \n",
       "Submission 5     0.13     0.00     0.00     0.00     0.86               5   \n",
       "\n",
       "                                                breakdown  \n",
       "Submission 1                                Topic 1: 0.99  \n",
       "Submission 2                 Topic 3: 0.61\\nTopic 5: 0.39  \n",
       "Submission 3                                Topic 5: 0.99  \n",
       "Submission 4  Topic 4: 0.59\\nTopic 2: 0.33\\nTopic 3: 0.08  \n",
       "Submission 5                 Topic 5: 0.86\\nTopic 1: 0.13  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign topic to each submision\n",
    "topicid = [\"Topic \" + str(i+1) for i in range(lda.n_components)]\n",
    "postid = [\"Submission \" + str(i+1) for i in range(len(df[\"Translated\"]))]\n",
    "\n",
    "df_topics_lda = pd.DataFrame(np.round(W,2), columns=topicid, index=postid)\n",
    "significanttopic = np.argmax(df_topics_lda.values, axis=1)+1\n",
    "\n",
    "df_topics_lda['dominant_topic'] = significanttopic\n",
    "df_topics_lda['breakdown'] = df_topics_lda.apply(lambda row: '\\n'.join([f'{col}: {row[col]}' \n",
    "                                                        for col in sorted(df_topics_lda.columns, key=lambda x: row[x], reverse=True) \n",
    "                                                        if row[col] > 0 and col != 'dominant_topic']), axis=1)\n",
    "df_topics_lda.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE Clustering and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2, random_state=120, perplexity=37, angle=0.8)\n",
    "tsne_result = tsne.fit_transform(df_topics_lda.iloc[:,:n_topics])\n",
    "\n",
    "# Create a new dataframe with t-SNE coordinates and cluster labels\n",
    "def split_text(text, max_length):\n",
    "  lines = textwrap.wrap(text, width=max_length, break_long_words=False)\n",
    "  return \"<br>\".join(lines)\n",
    "\n",
    "df_topics_cluster = pd.DataFrame({\n",
    "                                  'LDA Topic': df_topics_lda.reset_index()['dominant_topic'].astype(str), # topics via LDA\n",
    "                                  'Hovertext': df[\"Translated\"],\n",
    "                                  'Breakdown': df_topics_lda.reset_index()['breakdown'],\n",
    "                                  'X': tsne_result[:, 0],\n",
    "                                  'Y': tsne_result[:, 1]\n",
    "                                })\n",
    "\n",
    "df_topics_cluster['Hovertext'] = df_topics_cluster['Hovertext'].apply(lambda x: split_text(x, 40))\n",
    "df_topics_cluster['Breakdown'] = df_topics_cluster['Breakdown'].str.replace('\\n','<br>')\n",
    "\n",
    "df_topics_cluster = pd.concat([df, df_topics_cluster], axis=1)\n",
    "\n",
    "df_top_10 = df_topics_cluster.sort_values('Engagements', ascending=False).groupby('LDA Topic').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics_cluster.to_csv(f\"../dataset/_compiled/Clustered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nutshell Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_topics_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot submissions as colored points\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf_topics_cluster\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLDA_Topic\u001b[39m\u001b[38;5;124m'\u001b[39m, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: pd\u001b[38;5;241m.\u001b[39mto_numeric(x, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m CB_color_cycle \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#377eb8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ff7f00\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#4daf4a\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#f781bf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#a65628\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#984ea3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#999999\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#785ef0\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#dede00\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m Design_Book\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      9\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#ffb000\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     10\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#785ef0\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#984ea3\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     19\u001b[0m             ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_topics_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot submissions as colored points\n",
    "df_topics_cluster.sort_values('LDA_Topic', key=lambda x: pd.to_numeric(x, errors='coerce'), inplace=True)\n",
    "\n",
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#785ef0', '#dede00']\n",
    "\n",
    "Design_Book=[\n",
    "              '#ffb000', \n",
    "              '#785ef0', \n",
    "              '#dc267f', \n",
    "              '#fe6100', \n",
    "              '#57c4ff', \n",
    "              '#00cc96', \n",
    "              '#bcbd21', \n",
    "              '#a65628', \n",
    "              '#dede00',\n",
    "              '#984ea3',\n",
    "            ]\n",
    "\n",
    "fig = px.scatter(df_topics_cluster, x='X', y='Y', color='LDA_Topic', \n",
    "                 title='Topic Clustering using LDA and t-SNE',\n",
    "                 hover_name='Hovertext',\n",
    "                 size='Engagements',\n",
    "                 color_discrete_sequence=Design_Book,\n",
    "                 hover_data={'X':False, 'Y':False, 'LDA_Topic':False, 'Hovertext':False, 'Breakdown':True})\n",
    "\n",
    "fig.add_annotation(\n",
    "    x=0,\n",
    "    y=-0.2*(1/10)-0.15,\n",
    "    text=\"Top 10 Most Frequent Keywords per Topic\",\n",
    "    showarrow=False,\n",
    "    xref='paper',\n",
    "    yref='paper',\n",
    "    align='left',\n",
    "    font=dict(color='white', family='Arial', size=16, weight='normal')\n",
    "  )\n",
    "\n",
    "for i, keyword in enumerate(df_topics['Keywords']):\n",
    "  fig.add_annotation(\n",
    "    x=0,\n",
    "    y=-0.2*(i/5)-0.25,\n",
    "    text=\"Topic %d: %s\"%(i+1, keyword.replace(' ', ', ')),\n",
    "    showarrow=False,\n",
    "    xref='paper',\n",
    "    yref='paper',\n",
    "    align='left',\n",
    "    font=dict(color=fig.data[i].marker['color'], family='Arial', size=16, weight='normal')\n",
    "  )\n",
    "\n",
    "SIZE_MULTIPLIER = 1\n",
    "\n",
    "fig.update_traces(mode='markers', \n",
    "                  opacity=1,\n",
    "                  marker=dict(\n",
    "                    sizemode='area',\n",
    "                    sizeref=2.*max(df['Engagements'])/((130*(SIZE_MULTIPLIER))**2), \n",
    "                    line_color='#1a181c',\n",
    "                    line_width=2),\n",
    "                  )\n",
    "\n",
    "fig.update_layout(height=1080*SIZE_MULTIPLIER,\n",
    "                  width=1480*SIZE_MULTIPLIER,\n",
    "                  xaxis=dict(\n",
    "                    gridwidth=2,\n",
    "                    title='',\n",
    "                    color='gray',\n",
    "                  ),\n",
    "                  yaxis=dict(\n",
    "                    gridwidth=2,\n",
    "                    title='',\n",
    "                    color='gray',\n",
    "                  ),\n",
    "                  margin=dict(b=360*SIZE_MULTIPLIER),\n",
    "                  title=dict(font=dict(color='white', family='Roboto', size=24, weight='bold')),\n",
    "                  showlegend=False, \n",
    "                  paper_bgcolor='#1a181c',\n",
    "                  plot_bgcolor='#1a181c',\n",
    "                )\n",
    "\n",
    "fig.update_xaxes(showline=True, \n",
    "                 linewidth=2, \n",
    "                 linecolor='#232024', \n",
    "                 gridcolor='#232024', \n",
    "                 zerolinecolor='#232024',\n",
    "                 title_font_color=\"white\")\n",
    "\n",
    "fig.update_yaxes(showline=True,\n",
    "                 linewidth=2, \n",
    "                 linecolor='#232024', \n",
    "                 gridcolor='#232024', \n",
    "                 zerolinecolor='#232024',\n",
    "                 title_font_color=\"#FFFFFF\")\n",
    "                \n",
    "\n",
    "# !pip install nbformat\n",
    "# restart kernel\n",
    "pio.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topics_cluster['Date'] = pd.to_datetime(df_topics_cluster['Timestamp'], unit='s').dt.to_period('M')\n",
    "# df_topics_cluster = df_topics_cluster.sort_values(by='Timestamp')\n",
    "\n",
    "# total_count = df_topics_cluster.groupby('Date').nunique()\n",
    "# total_count = total_count['Submission'].cumsum()\n",
    "# cumulative_count = df_topics_cluster.groupby(['Date','Cluster']).nunique()\n",
    "# cumulative_count = cumulative_count.pivot_table('Submission', 'Date', 'Cluster').fillna(0).cumsum()\n",
    "\n",
    "# rel_freq = cumulative_count.div(total_count, axis=0)\n",
    "# rel_freq = rel_freq.stack(0).reset_index()\n",
    "# rel_freq.columns = ['Date', 'Cluster', 'Relative Frequency']\n",
    "# rel_freq['Date'] = [x.strftime('%b %Y') for x in rel_freq['Date']]\n",
    "\n",
    "# monthly_count = df_topics_cluster.groupby(['Date','Cluster']).size()\n",
    "# monthly_count = monthly_count.reset_index()\n",
    "# monthly_count.columns = ['Date', 'Cluster', 'Frequency']\n",
    "# monthly_count['Date'] = [x.strftime('%b %Y') for x in monthly_count['Date']]\n",
    "\n",
    "# cumulative_count = cumulative_count.stack(0).reset_index()\n",
    "# cumulative_count.columns = ['Date', 'Cluster', 'Frequency']\n",
    "# cumulative_count['Date'] = [x.strftime('%b %Y') for x in cumulative_count['Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = px.area(monthly_count, x='Date', y='Frequency', color='Cluster',\n",
    "#               title='Relative Frequency vs Time',\n",
    "#               color_discrete_sequence=Design_Book,\n",
    "#               labels={'Relative_Frequency': 'Relative Frequency', 'Date': 'Date'}\n",
    "#             )\n",
    "\n",
    "# fig.update_xaxes(nticks=7)\n",
    "\n",
    "# fig.update_layout(height=1080,\n",
    "#                   width=2160,\n",
    "#                   xaxis=dict(\n",
    "#                     gridwidth=2,\n",
    "#                     title='',\n",
    "#                     color='gray',\n",
    "#                   ),\n",
    "#                   yaxis=dict(\n",
    "#                     gridwidth=2,\n",
    "#                     title='',\n",
    "#                     color='gray',\n",
    "#                   ),\n",
    "#                   title=dict(font=dict(color='white', family='Roboto', size=24, weight='bold')),\n",
    "#                   legend=dict(title=\"Topic\", font=dict(color='white', family='Roboto', size=16, weight='normal')),\n",
    "#                   paper_bgcolor='#1a181c',\n",
    "#                   plot_bgcolor='#1a181c',\n",
    "#                 )\n",
    "\n",
    "# fig.update_xaxes(showline=True, \n",
    "#                  linewidth=2, \n",
    "#                  linecolor='#232024', \n",
    "#                  gridcolor='#232024', \n",
    "#                  zerolinecolor='#232024',\n",
    "#                  title_font_color=\"white\")\n",
    "\n",
    "# fig.update_yaxes(showline=True,\n",
    "#                  linewidth=2, \n",
    "#                  linecolor='#232024', \n",
    "#                  gridcolor='#232024', \n",
    "#                  zerolinecolor='#232024',\n",
    "#                  title_font_color=\"#FFFFFF\")\n",
    "\n",
    "# # Show the plot\n",
    "# # fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".scrape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
